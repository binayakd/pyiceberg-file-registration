{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfda232-5c35-4af0-8767-f8eb9efdb49a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In my [last post](https://binayakd.tech/posts/2024-08-30-exploring-iceberg/), I explored the fundamentals of how to create Apache Iceberg tables, using various catalogs, and how to use Spark and Trino to write and read data into and from these Iceberg tables. That involved using Spark as the the Iceberg client to write data into Iceberg table. \n",
    "\n",
    "However, in the case that data is already in object storage, following this process to create Iceberg tables, would involve a full migration (read, write, delete) of the data, which can prove time consuming and costly for large datasets. \n",
    "\n",
    "What we need is a workflow similar to [Hive's External tables](https://cwiki.apache.org/confluence/display/Hive/Managed+vs.+External+Tables), where writing and updating of the data is managed by an external process (or managed by a preexisting pipeline), and the Iceberg tables is the metadata layer, allowing querying of the data. \n",
    "\n",
    "This very problem has been addressed before in [this article](https://medium.com/inquery-data/registering-s3-files-into-apache-iceberg-tables-without-the-rewrites-3c087cb01658). However, that article used the Iceberg Java APIs, and is over one year old as of writing this, and proved to be somewhat cumbersome. \n",
    "\n",
    "Fortunately Pyiceberg, has come to the rescue to provide a more straightforward way to achieve this. Specifically, we can use the [`add_files`](https://py.iceberg.apache.org/api/#add-fields) method to register parquet files to a Iceberg table without rewrites. \n",
    "\n",
    "In this post, I will be essentially be following the Pyiceberg [Getting started tutorial](https://py.iceberg.apache.org/) with the difference being, I will being using Minio as the object storage, and using the `add_files` function, instead of appending (writing) the data.\n",
    "\n",
    "For this we need to setup Minio, and and Postgres as the backend for the Iceberg SQL catalog, which we can conveniently setup using a Docker compose file (found in this repo). You can of courses also just use files in local file system, and SQLite backed catalog, but that does not properly show the benefits of this workflow, which is to be able to migrate existing data in object storage to Iceberg format, without doing expensive rewrites. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f1987",
   "metadata": {},
   "source": [
    "## Test data setup\n",
    "We will be using the classic NYC Taxi datasets for these tests. So we download the set for January 2024, save it to our local filesystem, in the test-data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb8b29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 47.6M  100 47.6M    0     0  4217k      0  0:00:11  0:00:11 --:--:-- 5225k\n"
     ]
    }
   ],
   "source": [
    "!curl https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet -o ./local-data/test-data/yellow_tripdata_2024-01.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0d6ca",
   "metadata": {},
   "source": [
    "Then we will simulate a data generation process, such as ELT pipeline to upload our Minio instance. We will use Polars to do this here, but we can just as easily be using something like Spark or Pandas. \n",
    "\n",
    "> Note: There is a bug in Pyiceberg 0.8.0, where  the `add_files` method raises a Exception if the parquet file does not have column statistics. It just so happens that the NYC Taxi dataset parquet files do not have column statics. This is also another reason why we have to read the files with Polars, and re-write it t0 Minio instead of uploading the file directly, as Polars will add the statistics by default. \n",
    "> \n",
    ">This should be fixed in Pyiceberg 0.8.1, with merger of [this pull request](https://github.com/apache/iceberg-python/pull/1354)\n",
    "\n",
    "First read the file from local file system into a polars dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a34e21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_parquet(\"./local-data/test-data/yellow_tripdata_2024-01.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4912fa0",
   "metadata": {},
   "source": [
    "We now need to convert downcast the nanosecond timestamp columns into microsecond, as PyIceberg only supports down to microseconds. There is a mechanism for PyIceberg to help us to do the casting automatically using a [configurations or environment variable](https://py.iceberg.apache.org/configuration/#nanoseconds-support), however this only works if we are writing to the Iceberg table directly, instead of adding existing files. \n",
    "\n",
    "Thus this has to be done manually. We first check which columns need casting by getting the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a77b2724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('VendorID', Int32),\n",
       "        ('tpep_pickup_datetime', Datetime(time_unit='ns', time_zone=None)),\n",
       "        ('tpep_dropoff_datetime', Datetime(time_unit='ns', time_zone=None)),\n",
       "        ('passenger_count', Int64),\n",
       "        ('trip_distance', Float64),\n",
       "        ('RatecodeID', Int64),\n",
       "        ('store_and_fwd_flag', String),\n",
       "        ('PULocationID', Int32),\n",
       "        ('DOLocationID', Int32),\n",
       "        ('payment_type', Int64),\n",
       "        ('fare_amount', Float64),\n",
       "        ('extra', Float64),\n",
       "        ('mta_tax', Float64),\n",
       "        ('tip_amount', Float64),\n",
       "        ('tolls_amount', Float64),\n",
       "        ('improvement_surcharge', Float64),\n",
       "        ('total_amount', Float64),\n",
       "        ('congestion_surcharge', Float64),\n",
       "        ('Airport_fee', Float64)])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd1766",
   "metadata": {},
   "source": [
    "From here we see that columns `tpep_pickup_datetime` and `tpep_dropoff_datetime` are of type `Datatime` with time unit \"ns\". So those are what needs to be casted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5cfa1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(pl.col(\"tpep_pickup_datetime\").dt.cast_time_unit(\"ms\"))\n",
    "df = df.with_columns(pl.col(\"tpep_dropoff_datetime\").dt.cast_time_unit(\"ms\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743de7bd",
   "metadata": {},
   "source": [
    "We check the schema again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1dd10c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('VendorID', Int32),\n",
       "        ('tpep_pickup_datetime', Datetime(time_unit='ms', time_zone=None)),\n",
       "        ('tpep_dropoff_datetime', Datetime(time_unit='ms', time_zone=None)),\n",
       "        ('passenger_count', Int64),\n",
       "        ('trip_distance', Float64),\n",
       "        ('RatecodeID', Int64),\n",
       "        ('store_and_fwd_flag', String),\n",
       "        ('PULocationID', Int32),\n",
       "        ('DOLocationID', Int32),\n",
       "        ('payment_type', Int64),\n",
       "        ('fare_amount', Float64),\n",
       "        ('extra', Float64),\n",
       "        ('mta_tax', Float64),\n",
       "        ('tip_amount', Float64),\n",
       "        ('tolls_amount', Float64),\n",
       "        ('improvement_surcharge', Float64),\n",
       "        ('total_amount', Float64),\n",
       "        ('congestion_surcharge', Float64),\n",
       "        ('Airport_fee', Float64)])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa0850",
   "metadata": {},
   "source": [
    "There is one more update we need to do to the data. In my [previous post](https://binayakd.tech/posts/2024-08-30-exploring-iceberg/#writing-the-data-to-iceberg-table), we found out that although this file is marked for 2024-01, it actually has some stray data from some other months. We need to remove those extra month's data, as this will cause issues when we try to add this file to the Iceberg table partitioned by month. \n",
    "\n",
    "This is because, since adding files does not modify the actual files, the process will not be able to split the files into the different partitioned parquet files, and also can't add a single file to multiple partitions.\n",
    "\n",
    "So we can use polars to do this filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5df7ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\n",
    "    (pl.col(\"tpep_pickup_datetime\").dt.year() == 2024) & (pl.col(\"tpep_pickup_datetime\").dt.month() == 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c23b57",
   "metadata": {},
   "source": [
    "And we check if the filtering worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "199f7df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>year</th><th>month</th></tr><tr><td>i32</td><td>i8</td></tr></thead><tbody><tr><td>2024</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 2)\n",
       "┌──────┬───────┐\n",
       "│ year ┆ month │\n",
       "│ ---  ┆ ---   │\n",
       "│ i32  ┆ i8    │\n",
       "╞══════╪═══════╡\n",
       "│ 2024 ┆ 1     │\n",
       "└──────┴───────┘"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df\n",
    " .with_columns(pl.col(\"tpep_pickup_datetime\").dt.year().alias(\"year\"))\n",
    " .with_columns(pl.col(\"tpep_pickup_datetime\").dt.month().alias(\"month\"))\n",
    " .unique(subset=[\"year\", \"month\"])\n",
    " .select(['year', 'month'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eeed56",
   "metadata": {},
   "source": [
    "We can now write it into Minio. For that, we first setup the storage options for Minio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6083cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "conn_data = { \n",
    "    'key': 'admin', \n",
    "    'secret': 'password', \n",
    "    'client_kwargs': { \n",
    "        'endpoint_url': 'http://localhost:9000' \n",
    "        }\n",
    "}\n",
    "fs = s3fs.S3FileSystem(**conn_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daab0c7",
   "metadata": {},
   "source": [
    "And finally write it to our desired bucket and location, with statistics enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f902e70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binayak/Dropbox/dev/my-github/pyiceberg-file-registration/.venv/lib/python3.12/site-packages/botocore/auth.py:424: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n"
     ]
    }
   ],
   "source": [
    "s3_path = \"s3://warehouse/data/yellow_tripdata_2024-01.parquet\"\n",
    "\n",
    "with fs.open(s3_path, \"wb\") as f:\n",
    "    df.write_parquet(f, statistics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cba0f4",
   "metadata": {},
   "source": [
    "## Creating an SQL Catalog\n",
    "As mentioned, we will be creating an SQL catalog, using the Postgres instance as the DB backend. We also include the Minio connection details for the Warehouse location. This should correspond to the object storage instance that contains the preexisting files we want to add to the Iceberg tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c713306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "\n",
    "catalog = SqlCatalog(\n",
    "    \"default\",\n",
    "    **{\n",
    "        \"uri\": \"postgresql+psycopg2://postgres:postgres@localhost:5432/postgres\",\n",
    "        \"warehouse\": \"s3://warehouse/iceberg\",\n",
    "        \"s3.endpoint\": \"http://localhost:9000\",\n",
    "        \"s3.access-key-id\": \"admin\",\n",
    "        \"s3.secret-access-key\": \"password\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d5af1a",
   "metadata": {},
   "source": [
    "## Creating the Iceberg Table\n",
    "\n",
    "Now that we have our catalog setup, we need to first create the table, with a defined schema. \n",
    "This schema can be gotten from the Parquet file directly, using PyArrow. \n",
    "\n",
    "First we create a filesystem object to let Pyarrow know how to connect to Minio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60538e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from pyarrow import fs\n",
    "\n",
    "\n",
    "minio = fs.S3FileSystem(\n",
    "    endpoint_override='localhost:9000',\n",
    "    access_key=\"admin\",\n",
    "    secret_key=\"password\",\n",
    "    scheme=\"http\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4898c",
   "metadata": {},
   "source": [
    "Then we read the file as a PyArrow table from the specific bucket and path, and the Minio filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "175d88f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pq.read_table(\n",
    "    \"warehouse/data/yellow_tripdata_2024-01.parquet\",\n",
    "    filesystem=minio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c9526",
   "metadata": {},
   "source": [
    "We can check what the schema actually looks like, to ensure its matches to what we wrote before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "406fcf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID: int32\n",
       "tpep_pickup_datetime: timestamp[ms]\n",
       "tpep_dropoff_datetime: timestamp[ms]\n",
       "passenger_count: int64\n",
       "trip_distance: double\n",
       "RatecodeID: int64\n",
       "store_and_fwd_flag: large_string\n",
       "PULocationID: int32\n",
       "DOLocationID: int32\n",
       "payment_type: int64\n",
       "fare_amount: double\n",
       "extra: double\n",
       "mta_tax: double\n",
       "tip_amount: double\n",
       "tolls_amount: double\n",
       "improvement_surcharge: double\n",
       "total_amount: double\n",
       "congestion_surcharge: double\n",
       "Airport_fee: double"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ad678",
   "metadata": {},
   "source": [
    "We now have enough setup to create the namespace and table.\n",
    "\n",
    "Creating the namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "03e1e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.create_namespace(\"nyc_taxi_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca495195",
   "metadata": {},
   "source": [
    "And then the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f9f4e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = catalog.create_table(\n",
    "    \"nyc_taxi_data.yellow_tripdata\",\n",
    "    schema=df.schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a1150",
   "metadata": {},
   "source": [
    "Now we add the partition field (column) by using `MonthTransform` on the `tpep_pickup_datetime` column, to have the data partitioned by month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8cd028d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binayak/Dropbox/dev/my-github/pyiceberg-file-registration/.venv/lib/python3.12/site-packages/pyiceberg/utils/deprecated.py:54: DeprecationWarning: Deprecated in 0.8.0, will be removed in 0.9.0. Table.identifier property is deprecated. Please use Table.name() function instead.\n",
      "  _deprecation_warning(deprecation_notice(deprecated_in, removed_in, help_message))\n",
      "/home/binayak/Dropbox/dev/my-github/pyiceberg-file-registration/.venv/lib/python3.12/site-packages/pyiceberg/utils/deprecated.py:54: DeprecationWarning: Deprecated in 0.8.0, will be removed in 0.9.0. Support for parsing catalog level identifier in Catalog identifiers is deprecated. Please refer to the table using only its namespace and its table name.\n",
      "  _deprecation_warning(deprecation_notice(deprecated_in, removed_in, help_message))\n",
      "/home/binayak/Dropbox/dev/my-github/pyiceberg-file-registration/.venv/lib/python3.12/site-packages/pyiceberg/utils/deprecated.py:54: DeprecationWarning: Deprecated in 0.8.0, will be removed in 0.9.0. Table.identifier property is deprecated. Please use Table.name() function instead.\n",
      "  _deprecation_warning(deprecation_notice(deprecated_in, removed_in, help_message))\n"
     ]
    }
   ],
   "source": [
    "from pyiceberg.transforms import MonthTransform\n",
    "\n",
    "with table.update_spec() as update_spec:\n",
    "    update_spec.add_field(\n",
    "        source_column_name=\"tpep_pickup_datetime\",\n",
    "        transform=MonthTransform(),\n",
    "        partition_field_name=\"tpep_pickup_datetime_month\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc1b89",
   "metadata": {},
   "source": [
    "## Adding Parquet File to Table\n",
    "\n",
    "Now that we have created the table, with the partition fields, we can finally add the parquet file to the table. First we reload the table reference by the table name, just in case we need to re-run this, as `create_table` method cannot be run multiple time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d1843be0-415e-4507-ad49-e7e55966e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = catalog.load_table(\"nyc_taxi_data.yellow_tripdata\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfcdf5",
   "metadata": {},
   "source": [
    "Now we use the `add_files` method to add the file. Since this method takes in a list, we have to setup the list with our one file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a78467a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binayak/Dropbox/dev/my-github/pyiceberg-file-registration/.venv/lib/python3.12/site-packages/pyiceberg/utils/deprecated.py:54: DeprecationWarning: Deprecated in 0.8.0, will be removed in 0.9.0. Table.identifier property is deprecated. Please use Table.name() function instead.\n",
      "  _deprecation_warning(deprecation_notice(deprecated_in, removed_in, help_message))\n",
      "/home/binayak/Dropbox/dev/my-github/pyiceberg-file-registration/.venv/lib/python3.12/site-packages/pyiceberg/utils/deprecated.py:54: DeprecationWarning: Deprecated in 0.8.0, will be removed in 0.9.0. Support for parsing catalog level identifier in Catalog identifiers is deprecated. Please refer to the table using only its namespace and its table name.\n",
      "  _deprecation_warning(deprecation_notice(deprecated_in, removed_in, help_message))\n",
      "/home/binayak/Dropbox/dev/my-github/pyiceberg-file-registration/.venv/lib/python3.12/site-packages/pyiceberg/utils/deprecated.py:54: DeprecationWarning: Deprecated in 0.8.0, will be removed in 0.9.0. Table.identifier property is deprecated. Please use Table.name() function instead.\n",
      "  _deprecation_warning(deprecation_notice(deprecated_in, removed_in, help_message))\n"
     ]
    }
   ],
   "source": [
    "table.add_files([\"s3://warehouse/data/yellow_tripdata_2024-01.parquet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c9852-e788-4b4f-95c4-3aca7a80159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet -o ./local-data/warehouse/bronze/yellow_tripdata_2024-02.parquet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
